python finetune/llama.py --model_size 7b --w_bits 4 --learning_rate 5e-4 --max_iter 25600 --lsq_start 10 --out_dir out --data_dir data/alpaca_llama-1 --checkpoint_dir checkpoints/path/to/lit_checkpoint
python finetune/lora.py --model_size 7b --w_bits 4 --learning_rate 5e-4 --max_iter 25600 --lsq_start 10 --out_dir out --data_dir data/alpaca_llama-2 --checkpoint_dir checkpoints/path/to/lit_checkpoint
